{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"FINAl_GNN.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"fRpi7mVxtdxu"},"source":["!pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n","!pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n","!pip install torch-cluster -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n","!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n","!pip install torch-geometric\n","!pip install arrow\n","!pip install metpy\n","!pip install bresenham"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3yTiFKyC50VN"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YJ5AHTyQtlU3"},"source":["config = dict(\n","    experiments = dict(\n","        metero_use = [\n","                      '2m_temperature',\n","               'boundary_layer_height',  # Comment out for no_BPL experiments.\n","               'k_index',\n","               'relative_humidity+950',\n","               'surface_pressure',\n","               'total_precipitation',\n","               'u_component_of_wind+950',\n","               'v_component_of_wind+950',\n","        ],\n","        save_npy =True,\n","  dataset_num = 3,\n","  model = 'GRU'\n","    ),\n"," \n","train = dict(\n","  batch_size = 8,\n","  epochs = 10,\n","  exp_repeat = 1,\n","  hist_len = 8,\n","  pred_len = 24,\n","  weight_decay = 0.0005,\n","  early_stop = 10,\n","  lr = 0.0005),\n","data = dict(\n","    metero_var = [  '2m_temperature',\n","               'boundary_layer_height',  # Comment out for no_BPL experiments.\n","               'k_index',\n","               'relative_humidity+950',\n","               'surface_pressure',\n","               'total_precipitation',\n","               'u_component_of_wind+950',\n","               'v_component_of_wind+950',\n","                            \n","        ]\n","),\n","dataset = dict(\n","    data_start = [[2018, 1, 1, 0, 0], 'GMT'],\n","  data_end = [[2020, 12, 30, 0, 0], 'GMT'],\n"," \n","one = dict(\n","    train_start = [[2018,1, 1], 'GMT'],\n","    train_end = [[2019,10, 31], 'GMT'],\n","    val_start = [[2019,11,1], 'GMT'],\n","    val_end = [[2020,8, 31], 'GMT'],\n","    test_start = [[2020,9, 1], 'GMT'],\n","    test_end = [[2020, 12, 30], 'GMT'] )\n","),\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bDNiz8XTuFt0"},"source":["import os\n","import sys\n","proj_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n","sys.path.append(proj_dir)\n","import numpy as np\n","import torch\n","from collections import OrderedDict\n","from scipy.spatial import distance\n","from torch_geometric.utils import dense_to_sparse, to_dense_adj\n","from geopy.distance import geodesic\n","from metpy.units import units\n","import metpy.calc as mpcalc\n","from bresenham import bresenham\n"," \n"," \n"," \n","# city_fp = os.path.join(proj_dir, '/latitude_longitude.txt')\n","city_fp=\"/content/drive/MyDrive/cleaned data/latitude_longitude.txt\"\n","# altitude_fp = os.path.join(proj_dir, '/altitude.npy')\n","altitude_fp =  \"/content/drive/MyDrive/cleaned data/altitude.npy\"\n"," \n","class Graph():\n","    def __init__(self):\n","        self.dist_thres = 4\n","        self.use_altitude = True\n","        self.altitude = self._load_altitude()\n","        self.nodes = self._gen_nodes()\n","        self.node_attr = self._add_node_attr()\n","        self.node_num = len(self.nodes)\n","        self.edge_index, self.edge_attr = self._gen_edges()\n","        self.edge_num = self.edge_index.shape[1]\n","        self.adj = to_dense_adj(torch.LongTensor(self.edge_index))[0]\n"," \n","    def _load_altitude(self):\n","        assert os.path.isfile(altitude_fp)\n","        np.load.__defaults__=(None, True, True, 'ASCII')\n","        altitude = np.load(altitude_fp)\n","        np.load.__defaults__=(None, False, True, 'ASCII')\n","        return altitude\n"," \n","    def _gen_nodes(self):\n","        nodes = OrderedDict()\n","        with open(city_fp, 'r') as f:\n","            for line in f:\n","                idx, station , lon, lat = line.rstrip('\\n').split('\\t')\n","                idx = int(idx)\n","                lon, lat = float(lon), float(lat)\n","                nodes.update({idx: {'station': station ,'lon': lon, 'lat': lat}})\n","        return nodes\n"," \n","    def _add_node_attr(self):\n","        node_attr = []\n","        altitude_arr = []\n","        for i in self.nodes:\n","            altitude = self.altitude[i]\n","            altitude_arr.append(altitude)\n","        altitude_arr = np.stack(altitude_arr)\n","        # print(altitude_arr)\n","        node_attr = np.stack([altitude_arr], axis=-1)\n","        # print(node_attr)\n","        return node_attr\n"," \n","    def traverse_graph(self):\n","        lons = []\n","        lats = []\n","        citys = []\n","        idx = []\n","        for i in self.nodes:\n","            idx.append(i)\n","            city = self.nodes[i]['station']\n","            lon, lat = self.nodes[i]['lon'], self.nodes[i]['lat']\n","            lons.append(lon)\n","            lats.append(lat)\n","            citys.append(city)\n","        return idx, citys, lons, lats\n"," \n","    def gen_lines(self):\n"," \n","        lines = []\n","        for i in range(self.edge_index.shape[1]):\n","            src, dest = self.edge_index[0, i], self.edge_index[1, i]\n","            src_lat, src_lon = self.nodes[src]['lat'], self.nodes[src]['lon']\n","            dest_lat, dest_lon = self.nodes[dest]['lat'], self.nodes[dest]['lon']\n","            lines.append(([src_lon, dest_lon], [src_lat, dest_lat]))\n","        return lines\n"," \n","    def _gen_edges(self):\n","        coords = []\n","        lonlat = {}\n","        for i in self.nodes:\n","            coords.append([self.nodes[i]['lon'], self.nodes[i]['lat']])\n","        dist = distance.cdist(coords, coords, 'euclidean')\n","        adj = np.ones((self.node_num, self.node_num), dtype=np.uint8)\n","        assert adj.shape == dist.shape\n","        dist = dist * adj\n","        edge_index, dist = dense_to_sparse(torch.tensor(dist))\n","        edge_index, dist = edge_index.numpy(), dist.numpy()\n","        direc_arr = []\n","        dist_kilometer = []\n","        for i in range(edge_index.shape[1]):\n","            src, dest = edge_index[0, i], edge_index[1, i]\n","            src_lat, src_lon = self.nodes[src]['lat'], self.nodes[src]['lon']\n","            dest_lat, dest_lon = self.nodes[dest]['lat'], self.nodes[dest]['lon']\n","            src_location = (src_lat, src_lon)\n","            dest_location = (dest_lat, dest_lon)\n","            dist_km = geodesic(src_location, dest_location).kilometers\n","            v, u = src_lat - dest_lat, src_lon - dest_lon\n"," \n","            u = u * units.meter / units.second\n","            v = v * units.meter / units.second\n","            direc = mpcalc.wind_direction(u, v,convention=\"to\")._magnitude\n"," \n","            direc_arr.append(direc)\n","            dist_kilometer.append(dist_km)\n"," \n","        direc_arr = np.stack(direc_arr)\n","        dist_arr = np.stack(dist_kilometer)\n","        attr = np.stack([dist_arr, direc_arr], axis=-1)\n","        return edge_index, attr"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"abEjl22LxHCB"},"source":["import os\n","import sys\n","import yaml\n","from datetime import datetime\n","import numpy as np\n","import arrow\n","import metpy.calc as mpcalc\n","from metpy.units import units\n","from torch.utils import data\n"," \n"," \n","class HazeData(data.Dataset):\n","    def __init__(self, graph,\n","                       hist_len=1,\n","                       pred_len=24,\n","                       dataset_num=1,\n","                       flag='Train',\n","                       ):\n"," \n","        if flag == 'Train':\n","            start_time_str = 'train_start'\n","            end_time_str = 'train_end'\n","        elif flag == 'Val':\n","            start_time_str = 'val_start'\n","            end_time_str = 'val_end'\n","        elif flag == 'Test':\n","            start_time_str = 'test_start'\n","            end_time_str = 'test_end'\n","        else:\n","            raise Exception('Wrong Flag!')\n","        self.start_time = self._get_time(config['dataset'][\"one\"][start_time_str])\n","        self.end_time = self._get_time(config['dataset'][\"one\"][end_time_str])\n","        self.data_start = self._get_time(config['dataset']['data_start'])\n","        self.data_end = self._get_time(config['dataset']['data_end'])\n","        # file_dir = os.path.join(proj_dir,\"/IndiaData_GNN.npy\")\n","        file_dir = \"/content/drive/MyDrive/CHINA_DATASET/INDIA_FINAL_COLAB_PRO.npy\"\n","        self.knowair_fp = file_dir\n","        self.graph = graph\n","        self._load_npy()\n","        self._gen_time_arr()\n","        self._process_time()\n","        self._process_feature()\n","        self.feature = np.float32(self.feature)\n","        self.pm25 = np.float32(self.pm25)\n","        self._calc_mean_std()\n","        seq_len = hist_len + pred_len\n","        self._add_time_dim(seq_len)\n","        self._norm()\n"," \n","    def _norm(self):\n","        self.feature = (self.feature - self.feature_mean) / self.feature_std\n","        self.pm25 = (self.pm25 - self.pm25_mean) / self.pm25_std\n"," \n"," \n","    def _add_time_dim(self, seq_len):\n","        def _add_t(arr, seq_len):\n","            t_len = arr.shape[0]\n","            assert t_len > seq_len\n","            arr_ts = []\n","            for i in range(seq_len, t_len):\n","                arr_t = arr[i-seq_len:i]\n","                arr_ts.append(arr_t)\n","            arr_ts = np.stack(arr_ts, axis=0)\n","            return arr_ts\n","        \n","        self.pm25 = _add_t(self.pm25, seq_len)\n","        # print(self.pm25.shape)\n","        self.feature = _add_t(self.feature, seq_len)\n","        self.time_arr = _add_t(self.time_arr, seq_len)\n","        print(self.time_arr.shape)\n"," \n","    def _calc_mean_std(self):\n","        self.feature_mean = np.mean(self.feature,axis=(0,1))\n","        self.feature_std = np.std(self.feature,axis=(0,1))\n","        self.wind_mean = self.feature_mean[-2:]\n","        self.wind_std = self.feature_std[-2:]\n","        self.pm25_mean = np.mean(self.pm25)\n","        self.pm25_std = np.std(self.pm25)\n"," \n","    def _process_feature(self):\n","        metero_var = config['data']['metero_var']\n","        metero_use = config['experiments']['metero_use']\n","        metero_idx = [metero_var.index(var) for var in metero_use]\n","        self.feature = self.feature[:,:,metero_idx]\n","        u = self.feature[:, :, -2] * units.meter / units.second\n","        v = self.feature[:, :, -1] * units.meter / units.second\n","        speed = 3.6 * mpcalc.wind_speed(u, v)._magnitude\n","        direc = mpcalc.wind_direction(u, v)._magnitude\n","        h_arr = []\n","        w_arr = []\n","        for i in self.time_arrow:\n","            h_arr.append(i.hour)\n","            w_arr.append(i.isoweekday())\n","        h_arr = np.stack(h_arr, axis=-1)\n","        w_arr = np.stack(w_arr, axis=-1)\n","        # print(h_arr.shape)\n","        # print(self.feature.shape)\n","        h_arr = np.repeat(h_arr[:, None], self.graph.node_num, axis=1)\n","        w_arr = np.repeat(w_arr[:, None], self.graph.node_num, axis=1)\n","        self.feature = np.concatenate([self.feature, h_arr[:, :, None], w_arr[:, :, None],\n","                                       speed[:, :, None], direc[:, :, None]\n","                                       ], axis=-1)\n"," \n","    def _process_time(self):\n","        start_idx = self._get_idx(self.start_time)\n","        end_idx = self._get_idx(self.end_time)\n","        self.pm25 = self.pm25[start_idx: end_idx+1, :]\n","        self.feature = self.feature[start_idx: end_idx+1, :]\n","        self.time_arr = self.time_arr[start_idx: end_idx+1]\n","        self.time_arrow = self.time_arrow[start_idx: end_idx + 1]\n","        \n","        \n","    def _gen_time_arr(self):\n","        self.time_arrow = []\n","        self.time_arr = []\n","        for time_arrow in arrow.Arrow.interval('hour', self.data_start, self.data_end.shift(hours=+3),3):\n","            # print(time_arrow[0])\n","            self.time_arrow.append(time_arrow[0])\n","            self.time_arr.append(time_arrow[0].timestamp())\n","        self.time_arr = np.stack(self.time_arr, axis=-1)\n","  \n","    def _load_npy(self):\n","        np.load.__defaults__=(None, True, True, 'ASCII')\n","        self.knowair = np.load(\"/content/drive/MyDrive/CHINA_DATASET/INDIA_FINAL_COLAB_PRO.npy\")\n","        np.load.__defaults__=(None, False, True, 'ASCII')\n","        self.feature = self.knowair[:,:,:-1]\n","        self.pm25 = self.knowair[:,:,-1:]\n"," \n","    def _get_idx(self, t):\n","        t0 = self.data_start\n","        return int((t.timestamp() - t0.timestamp()) / (60 * 60 * 3))\n"," \n","    def _get_time(self, time_yaml):\n","        arrow_time = arrow.get(datetime(*time_yaml[0]), time_yaml[1])\n","        return arrow_time\n","    def __len__(self):\n","        return len(self.pm25)\n"," \n","    def __getitem__(self, index):\n","        return self.pm25[index], self.feature[index], self.time_arr[index]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n-oVSq_0uQbU"},"source":["graph = Graph()\n","train_data = HazeData(graph,8,24,\"one\",flag='Train')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jjfRvs3oyuJY"},"source":["val_data = HazeData(graph,8,24,\"one\",flag='Val')\n","test_data = HazeData(graph,8,24,\"one\",flag='Test')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lrhTN85ehIID"},"source":["from torch import nn\n","from torch.nn import Sequential, Linear, Sigmoid\n","\n","\n","class MLP(nn.Module):\n","    def __init__(self, hist_len, pred_len, in_dim):\n","        super(MLP, self).__init__()\n","        self.hist_len = hist_len\n","        self.pred_len = pred_len\n","        self.in_dim = in_dim\n","        self.hid_dim = 16\n","        self.out_dim = 1\n","        self.graph_mlp_out = 1\n","        self.graph_mlp_hid = 1\n","        self.fc_in = nn.Linear(self.in_dim, self.hid_dim)\n","        self.fc_out = nn.Linear(self.hid_dim, self.out_dim)\n","        self.mlp = Sequential(Linear(self.hid_dim, self.hid_dim),\n","                                   Sigmoid(),\n","                                    Linear(self.hid_dim, self.hid_dim),\n","                                    Sigmoid()\n","                                    )\n","\n","    def forward(self, pm25_hist, feature):\n","        pm25_pred = []\n","        xn = pm25_hist[:, -1]\n","        for i in range(self.pred_len):\n","            x = torch.cat((xn, feature[:, self.hist_len+i]), dim=-1)\n","            x = self.fc_in(x)\n","            x = self.mlp(x)\n","            xn = self.fc_out(x)\n","            pm25_pred.append(xn)\n","        pm25_pred = torch.stack(pm25_pred, dim=1)\n","\n","        return pm25_pred"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mYrcAvwRyuF3"},"source":["import numpy as np\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","class LSTMCell(nn.Module):\n","\n","    def __init__(self, input_size, hidden_size, bias=True):\n","        super(LSTMCell, self).__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.bias = bias\n","        self.x2h = nn.Linear(input_size, 4 * hidden_size, bias=bias)\n","        self.h2h = nn.Linear(hidden_size, 4 * hidden_size, bias=bias)\n","        self.reset_parameters()\n","\n","    def reset_parameters(self):\n","        std = 1.0 / np.sqrt(self.hidden_size)\n","        for w in self.parameters():\n","            w.data.uniform_(-std, std)\n","\n","    def forward(self, x, hidden):\n","        hx, cx = hidden\n","\n","        x = x.view(-1, x.size(-1))\n","\n","        gates = self.x2h(x) + self.h2h(hx)\n","\n","        gates = gates.squeeze()\n","\n","        ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)\n","\n","        ingate = torch.sigmoid(ingate)\n","        forgetgate = torch.sigmoid(forgetgate)\n","        cellgate = torch.tanh(cellgate)\n","        outgate = torch.sigmoid(outgate)\n","\n","        cy = torch.mul(cx, forgetgate) + torch.mul(ingate, cellgate)\n","\n","        hy = torch.mul(outgate, torch.tanh(cy))\n","\n","        return (hy, cy)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x7XogjAfyuDI"},"source":["import torch\n","from torch import nn\n","import numpy as np\n","# from model.cells import LSTMCell\n","class GRUCell(nn.Module):\n","\n","    def __init__(self, input_size, hidden_size, bias=True):\n","        super(GRUCell, self).__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.bias = bias\n","        self.x2h = nn.Linear(input_size, 3 * hidden_size, bias=bias)\n","        self.h2h = nn.Linear(hidden_size, 3 * hidden_size, bias=bias)\n","        self.reset_parameters()\n","\n","    def reset_parameters(self):\n","        std = 1.0 / np.sqrt(self.hidden_size)\n","        for w in self.parameters():\n","            w.data.uniform_(-std, std)\n","\n","    def forward(self, x, hidden):\n","        x = x.view(-1, x.size(-1))\n","\n","        gate_x = self.x2h(x)\n","        gate_h = self.h2h(hidden)\n","\n","        gate_x = gate_x.squeeze()\n","        gate_h = gate_h.squeeze()\n","\n","        i_r, i_i, i_n = gate_x.chunk(3, 1)\n","        h_r, h_i, h_n = gate_h.chunk(3, 1)\n","\n","        resetgate = torch.sigmoid(i_r + h_r)\n","        inputgate = torch.sigmoid(i_i + h_i)\n","        newgate = torch.tanh(i_n + (resetgate * h_n))\n","\n","        hy = newgate + inputgate * (hidden - newgate)\n","\n","        return hy\n","\n","\n","class LSTM(nn.Module):\n","    def __init__(self, hist_len, pred_len, in_dim, city_num, batch_size, device):\n","        super(LSTM, self).__init__()\n","        self.device = device\n","        self.hist_len = hist_len\n","        self.pred_len = pred_len\n","        self.city_num = city_num\n","        self.batch_size = batch_size\n","        self.in_dim = in_dim\n","        self.hid_dim = 32\n","        self.out_dim = 1\n","        self.fc_in = nn.Linear(self.in_dim, self.hid_dim)\n","        self.fc_out = nn.Linear(self.hid_dim, self.out_dim)\n","        self.lstm_cell = LSTMCell(self.hid_dim, self.hid_dim)\n","\n","    def forward(self, pm25_hist, feature):\n","        pm25_pred = []\n","        h0 = torch.zeros(self.batch_size * self.city_num, self.hid_dim).to(self.device)\n","        hn = h0\n","        c0 = torch.zeros(self.batch_size * self.city_num, self.hid_dim).to(self.device)\n","        cn = c0\n","        xn = pm25_hist[:, -1]\n","        for i in range(self.pred_len):\n","            x = torch.cat((xn, feature[:, self.hist_len+i]), dim=-1)\n","            x = self.fc_in(x)\n","            hn, cn = self.lstm_cell(x, (hn, cn))\n","            xn = hn.view(self.batch_size, self.city_num, self.hid_dim)\n","            xn = self.fc_out(xn)\n","            pm25_pred.append(xn)\n","        pm25_pred = torch.stack(pm25_pred, dim=1)\n","        return pm25_pred"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yYMw-JTMyt_7"},"source":["# from model.cells import LSTMCell\n","import torch.nn.functional as F\n","from torch_geometric.nn import ChebConv\n","import torch\n","from torch import nn\n","\n","\n","class GC_LSTM(nn.Module):\n","    def __init__(self, hist_len, pred_len, in_dim, city_num, batch_size, device, edge_index):\n","        super(GC_LSTM, self).__init__()\n","        self.edge_index = torch.LongTensor(edge_index)\n","        self.edge_index = self.edge_index.view(2, 1, -1).repeat(1, batch_size, 1) + torch.arange(batch_size).view(1, -1, 1) * batch_size\n","        self.edge_index = self.edge_index.view(2, -1)\n","        self.device = device\n","        self.hist_len = hist_len\n","        self.pred_len = pred_len\n","        self.city_num = city_num\n","        self.batch_size = batch_size\n","        self.in_dim = in_dim\n","        self.hid_dim = 32\n","        self.out_dim = 1\n","        self.gcn_out = 1\n","        self.conv = ChebConv(self.in_dim, self.gcn_out, K=2)\n","        self.lstm_cell = LSTMCell(self.in_dim + self.gcn_out, self.hid_dim)\n","        self.fc_out = nn.Linear(self.hid_dim, self.out_dim)\n","\n","    def forward(self, pm25_hist, feature):\n","        self.edge_index = self.edge_index.to(self.device)\n","        pm25_pred = []\n","        h0 = torch.zeros(self.batch_size * self.city_num, self.hid_dim).to(self.device)\n","        hn = h0\n","        c0 = torch.zeros(self.batch_size * self.city_num, self.hid_dim).to(self.device)\n","        cn = c0\n","        xn = pm25_hist[:, -1]\n","        for i in range(self.pred_len):\n","            x = torch.cat((xn, feature[:, self.hist_len+i]), dim=-1)\n","            x_gcn = x.contiguous()\n","            x_gcn = x_gcn.view(self.batch_size * self.city_num, -1)\n","            x_gcn = torch.sigmoid(self.conv(x_gcn, self.edge_index))\n","            x_gcn = x_gcn.view(self.batch_size, self.city_num, -1)\n","            x = torch.cat((x, x_gcn), dim=-1)\n","            hn, cn = self.lstm_cell(x, (hn, cn))\n","            xn = hn.view(self.batch_size, self.city_num, self.hid_dim)\n","            xn = self.fc_out(xn)\n","            pm25_pred.append(xn)\n","\n","        pm25_pred = torch.stack(pm25_pred, dim=1)\n","\n","        return pm25_pred"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5PfUeUxJyt3q"},"source":["import torch\n","from torch import nn\n","# from model.cells import GRUCell\n","from torch.nn import Sequential, Linear, Sigmoid\n","import numpy as np\n","from torch_scatter import scatter_add#, scatter_sub  # no scatter sub in lastest PyG\n","from torch.nn import functional as F\n","from torch.nn import Parameter\n","\n","\n","class GraphGNN(nn.Module):\n","    def __init__(self, device, edge_index, edge_attr, in_dim, out_dim, wind_mean, wind_std):\n","        super(GraphGNN, self).__init__()\n","        self.device = device\n","        self.edge_index = torch.LongTensor(edge_index).to(self.device)\n","        self.edge_attr = torch.Tensor(np.float32(edge_attr))\n","        self.edge_attr_norm = (self.edge_attr - self.edge_attr.mean(dim=0)) / self.edge_attr.std(dim=0)\n","        self.w = Parameter(torch.rand([1]))\n","        self.b = Parameter(torch.rand([1]))\n","        self.wind_mean = torch.Tensor(np.float32(wind_mean)).to(self.device)\n","        self.wind_std = torch.Tensor(np.float32(wind_std)).to(self.device)\n","        e_h = 32\n","        e_out = 30\n","        n_out = out_dim\n","        self.edge_mlp = Sequential(Linear(in_dim * 2 + 2 + 1, e_h),\n","                                   Sigmoid(),\n","                                   Linear(e_h, e_out),\n","                                   Sigmoid(),\n","                                   )\n","        self.node_mlp = Sequential(Linear(e_out, n_out),\n","                                   Sigmoid(),\n","                                   )\n","\n","    def forward(self, x):\n","        self.edge_index = self.edge_index.to(self.device)\n","        self.edge_attr = self.edge_attr.to(self.device)\n","        self.w = self.w.to(self.device)\n","        self.b = self.b.to(self.device)\n","\n","        edge_src, edge_target = self.edge_index\n","        node_src = x[:, edge_src]\n","        node_target = x[:, edge_target]\n","\n","        src_wind = node_src[:,:,-2:] * self.wind_std[None,None,:] + self.wind_mean[None,None,:]\n","        src_wind_speed = src_wind[:, :, 0]\n","        src_wind_direc = src_wind[:,:,1]\n","        self.edge_attr_ = self.edge_attr[None, :, :].repeat(node_src.size(0), 1, 1)\n","        city_dist = self.edge_attr_[:,:,0]\n","        city_direc = self.edge_attr_[:,:,1]\n","\n","        theta = torch.abs(city_direc - src_wind_direc)\n","        edge_weight = F.relu(3 * src_wind_speed * torch.cos(theta) / city_dist)\n","        edge_weight = edge_weight.to(self.device)\n","        edge_attr_norm = self.edge_attr_norm[None, :, :].repeat(node_src.size(0), 1, 1).to(self.device)\n","        out = torch.cat([node_src, node_target, edge_attr_norm, edge_weight[:,:,None]], dim=-1)\n","\n","        out = self.edge_mlp(out)\n","        out_add = scatter_add(out, edge_target, dim=1, dim_size=x.size(1))\n","        # out_sub = scatter_sub(out, edge_src, dim=1, dim_size=x.size(1))\n","        out_sub = scatter_add(out.neg(), edge_src, dim=1, dim_size=x.size(1))  # For higher version of PyG.\n","\n","        out = out_add + out_sub\n","        out = self.node_mlp(out)\n","\n","        return out\n","\n","\n","class PM25_GNN(nn.Module):\n","    def __init__(self, hist_len, pred_len, in_dim, city_num, batch_size, device, edge_index, edge_attr, wind_mean, wind_std):\n","        super(PM25_GNN, self).__init__()\n","\n","        self.device = device\n","        self.hist_len = hist_len\n","        self.pred_len = pred_len\n","        self.city_num = city_num\n","        self.batch_size = batch_size\n","\n","        self.in_dim = in_dim\n","        self.hid_dim = 64\n","        self.out_dim = 1\n","        self.gnn_out = 13\n","\n","        self.fc_in = nn.Linear(self.in_dim, self.hid_dim)\n","        self.graph_gnn = GraphGNN(self.device, edge_index, edge_attr, self.in_dim, self.gnn_out, wind_mean, wind_std)\n","        self.gru_cell = GRUCell(self.in_dim + self.gnn_out, self.hid_dim)\n","        self.fc_out = nn.Linear(self.hid_dim, self.out_dim)\n","\n","    def forward(self, pm25_hist, feature):\n","        pm25_pred = []\n","        h0 = torch.zeros(self.batch_size * self.city_num, self.hid_dim).to(self.device)\n","        hn = h0\n","        xn = pm25_hist[:, -1]\n","        for i in range(self.pred_len):\n","            x = torch.cat((xn, feature[:, self.hist_len + i]), dim=-1)\n","\n","            xn_gnn = x\n","            xn_gnn = xn_gnn.contiguous()\n","            xn_gnn = self.graph_gnn(xn_gnn)\n","            x = torch.cat([xn_gnn, x], dim=-1)\n","\n","            hn = self.gru_cell(x, hn)\n","            xn = hn.view(self.batch_size, self.city_num, self.hid_dim)\n","            xn = self.fc_out(xn)\n","            pm25_pred.append(xn)\n","\n","        pm25_pred = torch.stack(pm25_pred, dim=1)\n","\n","        return pm25_pred"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZzqdcQkEOquV"},"source":["from torch import nn\n","# from model.cells import GRUCell\n","from torch.nn import Sequential, Linear, Sigmoid\n","\n","\n","class nodesFC_GRU(nn.Module):\n","    def __init__(self, hist_len, pred_len, in_dim, city_num, batch_size, device):\n","        super(nodesFC_GRU, self).__init__()\n","        self.device = device\n","        self.hist_len = hist_len\n","        self.pred_len = pred_len\n","        self.city_num = city_num\n","        self.batch_size = batch_size\n","        self.in_dim = in_dim\n","        self.hid_dim = 32\n","        self.out_dim = 1\n","        self.graph_mlp_out = 1\n","        self.fc_out = nn.Linear(self.hid_dim, self.out_dim)\n","        self.gru_cell = GRUCell(self.in_dim + self.graph_mlp_out, self.hid_dim)\n","        self.graph_mlp = Sequential(Linear(self.city_num * self.in_dim, self.city_num * self.graph_mlp_out),\n","                                   Sigmoid())\n","\n","    def forward(self, pm25_hist, feature):\n","        pm25_pred = []\n","        h0 = torch.zeros(self.batch_size * self.city_num, self.hid_dim).to(self.device)\n","        hn = h0\n","        xn = pm25_hist[:, -1]\n","        for i in range(self.pred_len):\n","            x = torch.cat((xn, feature[:, self.hist_len+i]), dim=-1)\n","            # nodes FC\n","            xn_gnn = x\n","            xn_gnn = xn_gnn.contiguous()\n","            xn_gnn = xn_gnn.view(self.batch_size, -1)\n","            xn_gnn = self.graph_mlp(xn_gnn)\n","            xn_gnn = xn_gnn.view(self.batch_size, self.city_num, 1)\n","            x = torch.cat([xn_gnn, x], dim=-1)\n","            # nodes FC\n","            hn = self.gru_cell(x, hn)\n","            xn = hn.view(self.batch_size, self.city_num, self.hid_dim)\n","            xn = self.fc_out(xn)\n","            pm25_pred.append(xn)\n","\n","        pm25_pred = torch.stack(pm25_pred, dim=1)\n","        return pm25_pred"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tLN0KCHdZDEO"},"source":["class GRU(nn.Module):\n","    def __init__(self, hist_len, pred_len, in_dim, city_num, batch_size, device):\n","        super(GRU, self).__init__()\n","        self.device = device\n","        self.hist_len = hist_len\n","        self.pred_len = pred_len\n","        self.city_num = city_num\n","        self.batch_size = batch_size\n","        self.in_dim = in_dim\n","        self.hid_dim = 32\n","        self.out_dim = 1\n","        self.fc_in = nn.Linear(self.in_dim, self.hid_dim)\n","        self.fc_out = nn.Linear(self.hid_dim, self.out_dim)\n","        self.gru_cell = GRUCell(self.hid_dim, self.hid_dim)\n","\n","    def forward(self, pm25_hist, feature):\n","        pm25_pred = []\n","        h0 = torch.zeros(self.batch_size * self.city_num, self.hid_dim).to(self.device)\n","        hn = h0\n","        xn = pm25_hist[:, -1]\n","        for i in range(self.pred_len):\n","            x = torch.cat((xn, feature[:, self.hist_len+i]), dim=-1)\n","            x = self.fc_in(x)\n","            hn = self.gru_cell(x, hn)\n","            xn = hn.view(self.batch_size, self.city_num, self.hid_dim)\n","            xn = self.fc_out(xn)\n","            pm25_pred.append(xn)\n","        pm25_pred = torch.stack(pm25_pred, dim=1)\n","        return pm25_pred"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lUKdDhtqQwpZ"},"source":["import torch.nn.functional as F\n","from torch_geometric.nn import ChebConv\n","import torch\n","from torch import nn\n","\n","\n","class GC_LSTM(nn.Module):\n","    def __init__(self, hist_len, pred_len, in_dim, city_num, batch_size, device, edge_index):\n","        super(GC_LSTM, self).__init__()\n","        self.edge_index = torch.LongTensor(edge_index)\n","        self.edge_index = self.edge_index.view(2, 1, -1).repeat(1, batch_size, 1) + torch.arange(batch_size).view(1, -1, 1) * batch_size\n","        self.edge_index = self.edge_index.view(2, -1)\n","        self.device = device\n","        self.hist_len = hist_len\n","        self.pred_len = pred_len\n","        self.city_num = city_num\n","        self.batch_size = batch_size\n","        self.in_dim = in_dim\n","        self.hid_dim = 32\n","        self.out_dim = 1\n","        self.gcn_out = 1\n","        self.conv = ChebConv(self.in_dim, self.gcn_out, K=2)\n","        self.lstm_cell = LSTMCell(self.in_dim + self.gcn_out, self.hid_dim)\n","        self.fc_out = nn.Linear(self.hid_dim, self.out_dim)\n","\n","    def forward(self, pm25_hist, feature):\n","        self.edge_index = self.edge_index.to(self.device)\n","        pm25_pred = []\n","        h0 = torch.zeros(self.batch_size * self.city_num, self.hid_dim).to(self.device)\n","        hn = h0\n","        c0 = torch.zeros(self.batch_size * self.city_num, self.hid_dim).to(self.device)\n","        cn = c0\n","        xn = pm25_hist[:, -1]\n","        for i in range(self.pred_len):\n","            x = torch.cat((xn, feature[:, self.hist_len+i]), dim=-1)\n","            x_gcn = x.contiguous()\n","            x_gcn = x_gcn.view(self.batch_size * self.city_num, -1)\n","            x_gcn = torch.sigmoid(self.conv(x_gcn, self.edge_index))\n","            x_gcn = x_gcn.view(self.batch_size, self.city_num, -1)\n","            x = torch.cat((x, x_gcn), dim=-1)\n","            hn, cn = self.lstm_cell(x, (hn, cn))\n","            xn = hn.view(self.batch_size, self.city_num, self.hid_dim)\n","            xn = self.fc_out(xn)\n","            pm25_pred.append(xn)\n","\n","        pm25_pred = torch.stack(pm25_pred, dim=1)\n","\n","        return pm25_pred"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pGp6eAJgzduQ"},"source":["import os\n","import sys\n","import arrow\n","import torch\n","from torch import nn\n","from tqdm import tqdm\n","import numpy as np\n","import pickle\n","import glob\n","import shutil\n","import torchvision.transforms as transforms\n","from torch.cuda.amp import GradScaler, autocast\n","\n","torch.set_num_threads(1)\n","use_cuda = torch.cuda.is_available()\n","# device = torch.device('cuda' if use_cuda else 'cpu')\n","device = \"cuda:0\"\n","\n","\n","# graph = Graph()\n","city_num = graph.node_num\n","\n","\n","batch_size = config['train']['batch_size']\n","epochs = config['train']['epochs']\n","hist_len = config['train']['hist_len']\n","pred_len = config['train']['pred_len']\n","weight_decay = config['train']['weight_decay']\n","early_stop = config['train']['early_stop']\n","lr = config['train']['lr']\n","\n","results_dir = \"/content/drive/MyDrive/CHINA_DATASET/PM25GNNRESULTS\"\n","dataset_num = 'one'\n","exp_model = \"PM25_GNN\"\n","exp_repeat = config['train']['exp_repeat']\n","save_npy = config['experiments']['save_npy']\n","criterion = nn.MSELoss()\n","in_dim = train_data.feature.shape[-1] + train_data.pm25.shape[-1]\n","wind_mean, wind_std = train_data.wind_mean, train_data.wind_std\n","pm25_mean, pm25_std = train_data.pm25_mean, train_data.pm25_std\n","\n","gradient_accumulations = 4\n","scaler = GradScaler()\n","\n","\n","def get_metric(predict_epoch, label_epoch):\n","    haze_threshold = 60\n","    predict_haze = predict_epoch >= haze_threshold\n","    predict_clear = predict_epoch < haze_threshold\n","    label_haze = label_epoch >= haze_threshold\n","    label_clear = label_epoch < haze_threshold\n","    hit = np.sum(np.logical_and(predict_haze, label_haze))\n","    miss = np.sum(np.logical_and(label_haze, predict_clear))\n","    falsealarm = np.sum(np.logical_and(predict_haze, label_clear))\n","    csi = hit / (hit + falsealarm + miss)\n","    pod = hit / (hit + miss)\n","    far = falsealarm / (hit + falsealarm)\n","    predict = predict_epoch[:,:,:,0].transpose((0,2,1))\n","    label = label_epoch[:,:,:,0].transpose((0,2,1))\n","    predict = predict.reshape((-1, predict.shape[-1]))\n","    label = label.reshape((-1, label.shape[-1]))\n","    mae = np.mean(np.mean(np.abs(predict - label), axis=1))\n","    rmse = np.mean(np.sqrt(np.mean(np.square(predict - label), axis=1)))\n","    return rmse, mae, csi, pod, far\n","\n","\n","def get_exp_info():\n","    exp_info =  '============== Train Info ==============\\n' + \\\n","                'Dataset number: %s\\n' % dataset_num + \\\n","                'Model: %s\\n' % exp_model + \\\n","                'Train: %s --> %s\\n' % (train_data.start_time, train_data.end_time) + \\\n","                'Val: %s --> %s\\n' % (val_data.start_time, val_data.end_time) + \\\n","                'Test: %s --> %s\\n' % (test_data.start_time, test_data.end_time) + \\\n","                'City number: %s\\n' % city_num + \\\n","                'Use metero: %s\\n' % config['experiments']['metero_use'] + \\\n","                'batch_size: %s\\n' % batch_size + \\\n","                'epochs: %s\\n' % epochs + \\\n","                'hist_len: %s\\n' % hist_len + \\\n","                'pred_len: %s\\n' % pred_len + \\\n","                'weight_decay: %s\\n' % weight_decay + \\\n","                'early_stop: %s\\n' % early_stop + \\\n","                'lr: %s\\n' % lr + \\\n","                '========================================\\n'\n","    return exp_info\n","\n","\n","def get_model():\n","    if exp_model == 'MLP':\n","        return MLP(hist_len, pred_len, in_dim)\n","    elif exp_model == 'LSTM':\n","        return LSTM(hist_len, pred_len, in_dim, city_num, batch_size, device)\n","    elif exp_model == 'GRU':\n","        return GRU(hist_len, pred_len, in_dim, city_num, batch_size, device)\n","    elif exp_model == 'nodesFC_GRU':\n","        return nodesFC_GRU(hist_len, pred_len, in_dim, city_num, batch_size, device)\n","    elif exp_model == 'GC_LSTM':\n","        return GC_LSTM(hist_len, pred_len, in_dim, city_num, batch_size, device, graph.edge_index)\n","    elif exp_model == 'PM25_GNN':\n","        return PM25_GNN(hist_len, pred_len, in_dim, city_num, batch_size, device, graph.edge_index, graph.edge_attr, wind_mean, wind_std)\n","    elif exp_model == 'PM25_GNN_nosub':\n","        return PM25_GNN_nosub(hist_len, pred_len, in_dim, city_num, batch_size, device, graph.edge_index, graph.edge_attr, wind_mean, wind_std)\n","    else:\n","        raise Exception('Wrong model name!')\n","\n","\n","def train(train_loader, model, optimizer):\n","    model.train()\n","    train_loss = 0\n","    print(train_loader)\n","    for batch_idx, data in tqdm(enumerate(train_loader)):\n","        pm25, feature, time_arr = data\n","        pm25 = pm25.to(device)\n","        feature = feature.to(device)\n","        pm25_label = pm25[:, hist_len:]\n","        pm25_hist = pm25[:, :hist_len]\n","        # pm25_pred = model(pm25_hist, feature)\n","        # loss = criterion(pm25_pred, pm25_label)\n","        # loss.backward()\n","        # optimizer.step()\n","        with autocast():\n","             pm25_pred = model(pm25_hist, feature)\n","             loss = criterion(pm25_pred, pm25_label)\n","        scaler.scale(loss / gradient_accumulations).backward()\n","        # optimizer.step()\n","        if (batch_idx + 1) % gradient_accumulations == 0:\n","              scaler.step(optimizer)\n","              scaler.update()\n","              optimizer.zero_grad()\n","        train_loss += loss.item()\n","    train_loss /= batch_idx + 1\n","    return train_loss\n","\n","\n","def val(val_loader, model):\n","    model.eval()\n","    val_loss = 0\n","    for batch_idx, data in tqdm(enumerate(val_loader)):\n","        pm25, feature, time_arr = data\n","        pm25 = pm25.to(device)\n","        feature = feature.to(device)\n","        pm25_label = pm25[:, hist_len:]\n","        pm25_hist = pm25[:, :hist_len]\n","        pm25_pred = model(pm25_hist, feature)\n","        loss = criterion(pm25_pred, pm25_label)\n","        val_loss += loss.item()\n","\n","    val_loss /= batch_idx + 1\n","    return val_loss\n","\n","\n","def test(test_loader, model):\n","    model.eval()\n","    predict_list = []\n","    label_list = []\n","    time_list = []\n","    test_loss = 0\n","    for batch_idx, data in enumerate(test_loader):\n","        pm25, feature, time_arr = data\n","        pm25 = pm25.to(device)\n","        feature = feature.to(device)\n","        # print(\"--------------------------------------------\")\n","        pm25_label = pm25[:, hist_len:]\n","        # print(pm25_label.shape)\n","        # print(\"-----------------------------------------\")\n","        pm25_hist = pm25[:, :hist_len]\n","        # print(pm25_hist.shape)\n","        pm25_pred = model(pm25_hist, feature)\n","        loss = criterion(pm25_pred, pm25_label)\n","        test_loss += loss.item()\n","\n","        pm25_pred_val = np.concatenate([pm25_hist.cpu().detach().numpy(), pm25_pred.cpu().detach().numpy()], axis=1) * pm25_std + pm25_mean\n","        pm25_label_val = pm25.cpu().detach().numpy() * pm25_std + pm25_mean\n","        predict_list.append(pm25_pred_val)\n","        label_list.append(pm25_label_val)\n","        time_list.append(time_arr.cpu().detach().numpy())\n","\n","    test_loss /= batch_idx + 1\n","\n","    predict_epoch = np.concatenate(predict_list, axis=0)\n","    label_epoch = np.concatenate(label_list, axis=0)\n","    time_epoch = np.concatenate(time_list, axis=0)\n","    predict_epoch[predict_epoch < 0] = 0\n","\n","    return test_loss, predict_epoch, label_epoch, time_epoch\n","\n","\n","def get_mean_std(data_list):\n","    data = np.asarray(data_list)\n","    return data.mean(), data.std()\n","\n","\n","def main():\n","    exp_info = get_exp_info()\n","    print(exp_info)\n","\n","    exp_time = arrow.now().format('YYYYMMDDHHmmss')\n","\n","    train_loss_list, val_loss_list, test_loss_list, rmse_list, mae_list, csi_list, pod_list, far_list = [], [], [], [], [], [], [], []\n","\n","    for exp_idx in range(exp_repeat):\n","        print('\\nNo.%2d experiment ~~~' % exp_idx)\n","        # transform = transforms.Compose([transforms.ToTensor()])\n","        train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True)\n","        val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=False, drop_last=True)\n","        test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False, drop_last=True)\n","\n","        model = get_model()\n","        model = model.to(device)\n","        model_name = type(model).__name__\n","\n","        print(str(model))\n","\n","        optimizer = torch.optim.RMSprop(model.parameters(), lr=lr, weight_decay=weight_decay)\n","\n","        exp_model_dir = os.path.join(results_dir, '%s_%s' % (hist_len, pred_len), str(dataset_num), model_name, str(exp_time), '%02d' % exp_idx)\n","        if not os.path.exists(exp_model_dir):\n","            os.makedirs(exp_model_dir)\n","        model_fp = os.path.join(exp_model_dir, 'model.pth')\n","\n","        val_loss_min = 100000\n","        best_epoch = 0\n","\n","        train_loss_, val_loss_ = 0, 0\n","\n","        for epoch in range(epochs):\n","            print('\\nTrain epoch %s:' % (epoch))\n","\n","            train_loss = train(train_loader, model, optimizer)\n","            val_loss = val(val_loader, model)\n","\n","            print('train_loss: %.4f' % train_loss)\n","            print('val_loss: %.4f' % val_loss)\n","\n","            if epoch - best_epoch > early_stop:\n","                break\n","\n","            if val_loss < val_loss_min:\n","                val_loss_min = val_loss\n","                best_epoch = epoch\n","                print('Minimum val loss!!!')\n","                torch.save(model.state_dict(), model_fp)\n","                print('Save model: %s' % model_fp)\n","\n","                test_loss, predict_epoch, label_epoch, time_epoch = test(test_loader, model)\n","                train_loss_, val_loss_ = train_loss, val_loss\n","                rmse, mae, csi, pod, far = get_metric(predict_epoch, label_epoch)\n","                print('Train loss: %0.4f, Val loss: %0.4f, Test loss: %0.4f, RMSE: %0.2f, MAE: %0.2f, CSI: %0.4f, POD: %0.4f, FAR: %0.4f' % (train_loss_, val_loss_, test_loss, rmse, mae, csi, pod, far))\n","\n","                if save_npy:\n","                    np.save(os.path.join(exp_model_dir, 'predict.npy'), predict_epoch)\n","                    np.save(os.path.join(exp_model_dir, 'label.npy'), label_epoch)\n","                    np.save(os.path.join(exp_model_dir, 'time.npy'), time_epoch)\n","\n","        train_loss_list.append(train_loss_)\n","        val_loss_list.append(val_loss_)\n","        test_loss_list.append(test_loss)\n","        rmse_list.append(rmse)\n","        mae_list.append(mae)\n","        csi_list.append(csi)\n","        pod_list.append(pod)\n","        far_list.append(far)\n","\n","        print('\\nNo.%2d experiment results:' % exp_idx)\n","        print(\n","            'Train loss: %0.4f, Val loss: %0.4f, Test loss: %0.4f, RMSE: %0.2f, MAE: %0.2f, CSI: %0.4f, POD: %0.4f, FAR: %0.4f' % (\n","            train_loss_, val_loss_, test_loss, rmse, mae, csi, pod, far))\n","\n","    exp_metric_str = '---------------------------------------\\n' + \\\n","                     'train_loss | mean: %0.4f std: %0.4f\\n' % (get_mean_std(train_loss_list)) + \\\n","                     'val_loss   | mean: %0.4f std: %0.4f\\n' % (get_mean_std(val_loss_list)) + \\\n","                     'test_loss  | mean: %0.4f std: %0.4f\\n' % (get_mean_std(test_loss_list)) + \\\n","                     'RMSE       | mean: %0.4f std: %0.4f\\n' % (get_mean_std(rmse_list)) + \\\n","                     'MAE        | mean: %0.4f std: %0.4f\\n' % (get_mean_std(mae_list)) + \\\n","                     'CSI        | mean: %0.4f std: %0.4f\\n' % (get_mean_std(csi_list)) + \\\n","                     'POD        | mean: %0.4f std: %0.4f\\n' % (get_mean_std(pod_list)) + \\\n","                     'FAR        | mean: %0.4f std: %0.4f\\n' % (get_mean_std(far_list))\n","\n","    metric_fp = os.path.join(os.path.dirname(exp_model_dir), 'metric.txt')\n","    with open(metric_fp, 'w') as f:\n","        f.write(exp_info)\n","        f.write(str(model))\n","        f.write(exp_metric_str)\n","\n","    print('=========================\\n')\n","    print(exp_info)\n","    print(exp_metric_str)\n","    print(str(model))\n","    print(metric_fp)\n","\n","\n","if __name__ == '__main__':\n","    main()"],"execution_count":null,"outputs":[]}]}